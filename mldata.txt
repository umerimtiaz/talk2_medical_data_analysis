Project information Data
The project aims to develop a conversational AI system to interact with a breast cancer prognosis repository,
generate explanations and predictions based on the data, and provide insights to healthcare professionals and patients.
The project will develop a conversational interface that allows users to interact with the LLM model and ask questions
about breast cancer prognosis. The interface will use natural language processing (NLP) techniques to understand
user input and generate responses based on the LLM model's predictions. This project will be hosted at umerai.com
at later stages of project completion,for general use.

Machine Learning Data
Meta Data
uci_id: 16
name: Breast Cancer Wisconsin (Prognostic)
repository_url: https://archive.ics.uci.edu/dataset/16/breast+cancer+wisconsin+prognostic
data_url: https://archive.ics.uci.edu/static/public/16/data.csv
abstract: Prognostic Wisconsin Breast Cancer Database
area: Health and Medicine
tasks: ['Classification', 'Regression']
characteristics: ['Multivariate']
num_instances: 198
num_features: 33
feature_types: ['Real']
demographics: []
target_col: ['Outcome']
index_col: ['ID']
has_missing_values: yes
missing_values_symbol: NaN
year_of_dataset_creation: 1995
last_updated: Sun Jan 14 2024
dataset_doi: 10.24432/C5GK50
creators: ['William Wolberg', 'W. Street', 'Olvi Mangasarian']
intro_paper: None
additional_info: {'summary': 'Each record represents follow-up data for one breast cancer case.  These are consecutive patients seen by Dr. Wolberg since 1984, and include only those cases exhibiting invasive breast cancer and no evidence of distant metastases at the time of diagnosis. \r\n\r\nThe first 30 features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image. A few of the images can be found at http://www.cs.wisc.edu/~street/images/\r\n\r\nThe separation described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, "Decision Tree Construction Via Linear Programming." Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree.  Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.\r\n\r\nThe actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: \r\n[K. P. Bennett and O. L. Mangasarian: "Robust Linear Programming Discrimination of Two Linearly Inseparable Sets", Optimization Methods and Software 1, 1992, 23-34].\r\n\r\nThe Recurrence Surface Approximation (RSA) method is a linear programming model which predicts Time To Recur using both recurrent and nonrecurrent cases.  See references (i) and (ii) above for details of the RSA method. \r\n\r\nThis database is also available through the UW CS ftp server:\r\n\r\nftp ftp.cs.wisc.edu\r\ncd math-prog/cpo-dataset/machine-learn/WPBC/', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '1) ID number\n2) Outcome (R = recur, N = nonrecur)\n3) Time (recurrence time if field 2 = R, disease-free time if \n\tfield 2\t= N)\n4-33) Ten real-valued features are computed for each cell nucleus:\n\n\ta) radius (mean of distances from center to points on the perimeter)\n\tb) texture (standard deviation of gray-scale values)\n\tc) perimeter\n\td) area\n\te) smoothness (local variation in radius lengths)\n\tf) compactness (perimeter^2 / area - 1.0)\n\tg) concavity (severity of concave portions of the contour)\n\th) concave points (number of concave portions of the contour)\n\ti) symmetry \n\tj) fractal dimension ("coastline approximation" - 1)\n\n34) Tumor size - diameter of the excised tumor in centimeters\n35) Lymph node status - number of positive axillary lymph nodes\nobserved at time of surgery', 'citation': None}


X.shape is (198, 33)

lymph_node_status has null value count 4
list of null value columns ['lymph_node_status']
lymph_node_status has null value count 0 after treating missing values as mean of the data
BEFORE  >> list of AI model parameters - all models together >> 
 LogisticRegression() {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
BEFORE  >> list of AI model parameters - all models together >> 
 SVC() {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}
BEFORE  >> list of AI model parameters - all models together >> 
 DecisionTreeClassifier() {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}
BEFORE  >> list of AI model parameters - all models together >> 
 RandomForestClassifier() {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}
BEFORE  >> list of AI model parameters - all models together >> 
 KNeighborsClassifier() {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
AFTER >> list of AI model parameters - all models together >> 
 [{'fit_intercept': [True, False]}, {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}, {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]}, {}, {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]}]
grid_search_model.best_params_ {'fit_intercept': False}
Saving Model: Logistic_Regression.joblib
grid_search_model.best_params_ {'C': 100, 'gamma': 0.01}
Saving Model: SVM.joblib
grid_search_model.best_params_ {'max_depth': 13}
Saving Model: Decision_Tree_Classifier.joblib
grid_search_model.best_params_ {}
Saving Model: Random_Forest_Classifier.joblib
grid_search_model.best_params_ {'n_neighbors': 1}
Saving Model: KNN.joblibLoading Model: Logistic_Regression.joblibRunning test score >>>> working
Working Successfull >>>> Logistic_Regression
Loading Model: SVM.joblibRunning test score >>>> working
Working Successfull >>>> SVM
Loading Model: Decision_Tree_Classifier.joblibRunning test score >>>> working
Working Successfull >>>> Decision_Tree_Classifier
Loading Model: Random_Forest_Classifier.joblibRunning test score >>>> working
Working Successfull >>>> Random_Forest_Classifier
Loading Model: KNN.joblibRunning test score >>>> working
Working Successfull >>>> KNN

sorted Model #accuracy scores:
 [['SVM', 0.8], ['Random_Forest_Classifier', 0.8], ['Decision_Tree_Classifier', 0.775], ['KNN', 0.775], ['Logistic_Regression', 0.625]]
sorted Model #f1 scores:
 [['SVM', 0.5], ['Decision_Tree_Classifier', 0.47058823529411764], ['KNN', 0.47058823529411764], ['Logistic_Regression', 0.4444444444444444], ['Random_Forest_Classifier', 0.3333333333333333]]
sorted Model #precision scores:
 [['SVM', 0.5], ['Random_Forest_Classifier', 0.5], ['Decision_Tree_Classifier', 0.4444444444444444], ['KNN', 0.4444444444444444], ['Logistic_Regression', 0.3157894736842105]]
sorted Model #recall scores:
 [['Logistic_Regression', 0.75], ['SVM', 0.5], ['Decision_Tree_Classifier', 0.5], ['KNN', 0.5], ['Random_Forest_Classifier', 0.25]]
 Confusion Matrix (CM) scores of Logistic_Regression as Numpy array 
 [[19 13]
 [ 2  6]]
True Positives as TP = 19
False Positives as FN = 13
True Positives as TP = 2
False Positives as FN = 6
 Confusion Matrix (CM) scores of SVM as Numpy array 
 [[28  4]
 [ 4  4]]
True Positives as TP = 28
False Positives as FN = 4
True Positives as TP = 4
False Positives as FN = 4
 Confusion Matrix (CM) scores of Decision_Tree_Classifier as Numpy array 
 [[27  5]
 [ 4  4]]
True Positives as TP = 27
False Positives as FN = 5
True Positives as TP = 4
False Positives as FN = 4
 Confusion Matrix (CM) scores of Random_Forest_Classifier as Numpy array 
 [[30  2]
 [ 6  2]]
True Positives as TP = 30
False Positives as FN = 2
True Positives as TP = 6
False Positives as FN = 2
 Confusion Matrix (CM) scores of KNN as Numpy array 
 [[27  5]
 [ 4  4]]
True Positives as TP = 27
False Positives as FN = 5
True Positives as TP = 4
False Positives as FN = 4